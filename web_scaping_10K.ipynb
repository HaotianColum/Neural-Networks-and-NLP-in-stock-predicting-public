{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Web scraping on Financial Statements (on progress)\n",
    "## Structure\n",
    "For web scraping, there are two kind of open data source we can get, either twits or 10-K(Q). The original idea here is to download 10-K by clk and use tfidf/BERT to get sentiment/features from 10-K. Then use stock yearly/quartly returns as label to do supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pprint\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cik Lookup Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_lookup = {\n",
    "    'AMZN': '0001018724',\n",
    "    'HD': '0000354950',\n",
    "    'NKE': '0000320187',\n",
    "    'MCD': '0000063908',\n",
    "    'SBUX': '0000887557',\n",
    "    'LOW': '0000060667',\n",
    "    'BKNG': '0001075531',\n",
    "    'TJX': '0000109198',\n",
    "    'TGT': '0000027419',\n",
    "    'GM': '0000040730',\n",
    "    'LVS': '0000850994',\n",
    "    'MAR': '0001048286',\n",
    "    'ROST': '0000745732',\n",
    "    'DG': '0000029534',\n",
    "    'VFC': '0001258370',\n",
    "    'F': '0000037996',\n",
    "    'ORLY': '0000898173',\n",
    "    'CCL': '0000815097',\n",
    "    'HLT': '0001593790',\n",
    "    'YUM': '0001041061',\n",
    "    'EBAY': '0001065088',\n",
    "    'AZO': '0000866787',\n",
    "    'RCL': '0000884887',\n",
    "    'APTV': '0001521332',\n",
    "    'CMG': '0001058090'} # not use\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get 10-ks\n",
    "give an example of AMZN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the class constructed, let's pull a list of filled 10-ks from the SEC for each company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import requests\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "def _get_most_recent_index_page(ticker, filing):\n",
    "    \"\"\"\n",
    "    This helper function return index_url by ticker and file_type\n",
    "    \"\"\"  \n",
    "    try:\n",
    "        page = requests.get(\"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=\" \n",
    "                        + ticker \n",
    "                        + \"&type=\" + filing + \"&owner=exclude&count=40&search_text=\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error when getting\", filing, \"file index page for\", ticker)\n",
    "        raise Exception(e)\n",
    "    \n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    table = soup.find(lambda tag: tag.name=='table' and tag.has_attr('summary') and tag['summary']==\"Results\")\n",
    "    if table is not None:\n",
    "        rows = table.findAll('tr')\n",
    "        if rows is not None:\n",
    "            doc_index_link = None\n",
    "            for tr in rows:\n",
    "                tds = tr.find_all('td')\n",
    "                row = [i.text for i in tds]\n",
    "                if len(row) >= 2 and filing == row[0]:   # the second element is the type of the file\n",
    "                    return '/'.join(['http://sec.gov'] + [tds[1].find('a')['href']])\n",
    "    print(\"Cannot find\", filing, \"file index page for\", ticker)\n",
    "    return None\n",
    "\n",
    "def _get_file_url_by_index_url(index_url, filing):\n",
    "    \"\"\"\n",
    "    This helper function return file_url by index_url and file_type\n",
    "    \"\"\"  \n",
    "    try:\n",
    "        page = requests.get(index_url)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error when getting url ticker\")\n",
    "        raise Exception(e)\n",
    "\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    table = soup.find(lambda tag: tag.name=='table' \n",
    "                      and tag.has_attr('summary') \n",
    "                      and tag['summary']==\"Document Format Files\")\n",
    "    if table is not None:\n",
    "        rows = table.findAll('tr')\n",
    "        if rows is not None:\n",
    "            doc_link = None\n",
    "            for tr in rows:\n",
    "                tds = tr.find_all('td')\n",
    "                row = [i.text for i in tds]\n",
    "                if len(row) >= 2 and filing in row[1]:   # the second element is the type of the file\n",
    "                    return '/'.join(['http://sec.gov'] + tds[2].find('a')['href'].split('/')[2:])\n",
    "    print(\"Index Page Found\")\n",
    "    return None\n",
    "\n",
    "def get_most_recent_file(ticker, filing):\n",
    "    \"\"\"\n",
    "    This function return pages by ticker and file_type\n",
    "    \"\"\"  \n",
    "    index_url = _get_most_recent_index_page(ticker, filing)\n",
    "    if index_url is None:\n",
    "        raise Exception(\"Cannot find index page\")\n",
    "    file_url = _get_file_url_by_index_url(index_url, filing)\n",
    "    if file_url is None:\n",
    "        raise Exception(\"Cannot find file\")\n",
    "    return requests.get(file_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pull the files using the `get_most_recent_file` function, and I choose to display one of the results. For displaying some of the data, we'll use the first ticker as an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, I remove the duplicates of headlines in CSV, resulting in 24/40W left (most are still missing)\n",
    "tickers = [w.lower() for w in pd.read_excel('./input/full_ticker_mapping.xlsx',engine='openpyxl')[\"ticker\"]]\n",
    "example_ticker = tickers[0] # amzn\n",
    "\n",
    "# for ticker, cik in cik_lookup.items():\n",
    "#     page[ticker] = get_most_recent_file(ticker, '10-K')\n",
    "page = get_most_recent_file(example_ticker, '10-K')\n",
    "# pprint.pprint(page[example_ticker])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<?xml version=\"1.0\" ?><!--XBRL Document Created with Wdesk from Workiva--><!--Copyright 2021 Workiva'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_consecutive_span(s):\n",
    "    \"\"\"\n",
    "    This function return pages by ticker and file_type\n",
    "    \"\"\"  \n",
    "    start = 0\n",
    "    pattern = '</span><span'\n",
    "    locations = []\n",
    "    while start != -1:\n",
    "        start = s.find(pattern, start)\n",
    "        if start != -1:\n",
    "            end = s.find('>', start + len(pattern))\n",
    "            locations.append((start, end))\n",
    "            start = end + 1\n",
    "    if len(locations) == 0:\n",
    "        return s\n",
    "    res = [s[0:locations[0][0]]]\n",
    "    for i in range(1, len(locations)):\n",
    "        res += [s[locations[i-1][1]+1:locations[i][0]]]\n",
    "    res += [s[locations[-1][1]+1:]]\n",
    "    return ''.join(res)\n",
    "\n",
    "html = remove_consecutive_span(page.content.decode('utf-8'))\n",
    "html[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Table of Contents UNITED STATES SECURITIES AND EXCHANGE COMMISSION Washington, D.C. 20549 __________'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_from_html(html):\n",
    "    invisible = set(['style', 'script', 'head', 'title', 'meta', '[document]'])\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    soup.find('ix:header').decompose()\n",
    "    elements = soup.findAll(text=True)\n",
    "    visible = []\n",
    "    for ele in elements:\n",
    "        if ele.parent.name not in invisible and not isinstance(ele, Comment):\n",
    "            visible.append(ele)\n",
    "    return u\" \".join(t.strip() for t in visible)\n",
    "visible_txt = text_from_html(html)\n",
    "visible_txt[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess\n",
    "This step takes a long string as input, then performs some pre-operations like delete companies tickers that will not be feasible in future feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/z20171126/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/z20171126/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess(message):\n",
    "    \"\"\"\n",
    "    This function takes a string as input, then performs these operations: \n",
    "        - lowercase\n",
    "        - remove ticker symbols \n",
    "        - removes punctuation\n",
    "        - tokenize by splitting the string on whitespace \n",
    "        - removes any single character tokens\n",
    "    \"\"\"    \n",
    "    # Lowercase headline\n",
    "    text = message.lower()\n",
    "    \n",
    "    # Replace URLs with a space in the message\n",
    "    text = re.sub(r'^http?:\\/\\/.*[\\r\\n]*', ' ', text, flags = re.MULTILINE)\n",
    "    \n",
    "    # Replace ticker symbols with a space. The ticker symbols are any stock symbol that starts with >.\n",
    "    text = re.sub (r'^>.*\\s',' ',text,flags=re.MULTILINE)\n",
    "\n",
    "    # Replace everything not a letter with a space\n",
    "    text = re.sub (r'[^a-zA-Z]',' ',text,flags=re.MULTILINE)\n",
    "    \n",
    "    # Tokenize by splitting the string on whitespace into a list of words\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "    # Lemmatize words using the WordNetLemmatizer. You can ignore any word that is not longer than one character.\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [wnl.lemmatize(t) for t in tokens if t not in tickers] # delete company tickers\n",
    "    \n",
    "    return tokens\n",
    "words = preprocess(visible_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the frequently use pronouns and propositions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronouns = set(['i', 'me', 'myself', 'mine', 'mine','my',\n",
    "                'we', 'us', 'ourself', 'ourselves', 'ours', 'our',\n",
    "                'you', 'yourself', 'yourselves', 'yours', 'your',\n",
    "                'thou', 'thee', 'thyself', 'thine', 'thy',\n",
    "                'ye', 'yeers', \"y\\'all\", 'youse', 'yeerselves',\n",
    "                'he', 'him', 'himself', 'his',\n",
    "                'she', 'her', 'herself', 'hers',\n",
    "                'it', 'itself', 'its'])\n",
    "propositions = set(['aboard','about','above','across','after','against','along','amid','among','around','as',\n",
    "                    'at','before','behind','below','beneath','beside','between','beyond','but','by','concerning','considering',\n",
    "                    'despite','down','during','except','following','for','from','in','inside','into','like','minus','near','next',\n",
    "                    'of','off','on','onto','opposite','out','outside','over','past','per','plus','regarding','round','save','since',\n",
    "                    'than','through','till','to','toward','under','underneath','unlike','until','up','upon','versus','via',\n",
    "                    'with','within','without'\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download NLP Corpora\n",
    "download the stopwords corpus for removing stopwords and count the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/z20171126/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('leas', 483), ('vehicl', 450), ('may', 415), ('oper', 389), ('energi', 388), ('decemb', 371), ('product', 354), ('million', 349), ('cost', 346), ('note', 336)]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "   \n",
    "ps = PorterStemmer() \n",
    "cnt = {}\n",
    "stem_to_source = {}\n",
    "\n",
    "for raw_word in words:\n",
    "    word = raw_word.lower()\n",
    "    word = re.sub(r\"[^a-z0-9'.-]\", \" \", word) # Include period since phrase like I.R.S should be considered legal.\n",
    "    if word[-1] == '.':\n",
    "        word = word[:-1]                        # Only remove the periods that work as the end of sentence.\n",
    "    word = word.strip()\n",
    "    pure_num=re.compile(\"^[0-9. -]*$\")\n",
    "    if (not bool(pure_num.match(word))) and word not in stopwords.words('english'):\n",
    "        stemmed_word = ps.stem(word)\n",
    "        if stemmed_word not in pronouns and stemmed_word not in propositions:\n",
    "            if stemmed_word in cnt:\n",
    "                cnt[stemmed_word] += 1\n",
    "                if word in stem_to_source[stemmed_word]:\n",
    "                    stem_to_source[stemmed_word][word] += 1\n",
    "                else:\n",
    "                    stem_to_source[stemmed_word][word] = 1\n",
    "            else:\n",
    "                cnt[stemmed_word] = 1\n",
    "                stem_to_source[stemmed_word] = {word: 1}\n",
    "\n",
    "# below is the first tenth frequent words in 10-K            \n",
    "print(sorted(cnt.items(), key=lambda item: -item[1])[:10]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Works\n",
    "using the word lists, we can generate sentiment bag of words from the 10-k documents. Or using the words, we can calculate the jaccard similarity on the bag of words and plot it over time by using `from sklearn.metrics import jaccard_similarity_score`. Or we can generate TFIDF from the 10-k documents. It seems trivial if we directly apply SentimentAnalysis package on 10K."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
